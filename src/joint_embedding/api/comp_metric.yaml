functionality:
  arguments:
    - name: --input_prediction
      __merge__: anndata_prediction.yaml
    - name: --input_solution
      __merge__: anndata_solution.yaml
    - name: --output
      __merge__: anndata_score.yaml
      direction: output
    - name: --debug
      type: boolean_true
      description: Verbose output for debugging.
  test_resources:
    - path: ../../../../resources_test
    - type: python_script
      path: generic_test.py
      text: |
        from os import path
        import subprocess
        import anndata as ad
        import pandas as pd
        import yaml

        ## VIASH START
        # This code block will be replaced by viash at runtime.
        par = {
          "input_prediction": "resources_test/joint_embedding/openproblems_bmmc_multiome_starter/openproblems_bmmc_multiome_starter.prediction.h5ad",
          "input_solution": "resources_test/joint_embedding/openproblems_bmmc_multiome_starter/openproblems_bmmc_multiome_starter.solution.h5ad",
          "output": "output.h5ad"
        }
        meta = { 'functionality_name': 'foo' }

        ## VIASH END

        input_prediction_path = "resources_test/common/joint_embedding/cite_random_prediction.h5ad"
        input_solution_path = "resources_test/common/joint_embedding/cite_solution.h5ad"
        output_path = "output.h5ad"
        # define some filenames
        with open(meta["config"], "r") as file:
                config = yaml.safe_load(file)

        cmd = [
            meta['executable'],
            "--input_prediction", input_prediction_path,
            "--input_solution", input_solution_path,
            "--output", output_path
        ]

        print("> Running method", flush=True)
        out = subprocess.run(cmd, capture_output=True, text=True, check=True).stdout

        print("> Checking whether output files were created", flush=True)
        assert path.exists(output_path)

        print("> Reading h5ad files", flush=True)
        input_prediction = ad.read_h5ad(input_prediction_path)
        input_solution = ad.read_h5ad(input_solution_path)
        output = ad.read_h5ad(output_path)

        # Create DF from metric config info
        metric_info = config['functionality']['info']['metrics']
        metric_meta = pd.DataFrame(metric_info)
        metric_meta = metric_meta.astype({'id': str, 'label': str, 'description': str, 'min': float, 'max': float, 'maximize': bool})
        print("> Checking contents of metric info", flush=True)
        assert 'id' in metric_meta
        assert 'min' in metric_meta
        assert 'max' in metric_meta
        assert 'maximize' in metric_meta

        print("> Checking .uns['dataset_id']", flush=True)
        assert 'dataset_id' in output.uns
        assert output.uns['dataset_id'] == input_prediction.uns['dataset_id']

        print("> Checking .uns['method_id']", flush=True)
        assert 'method_id' in output.uns
        assert output.uns['method_id'] == input_prediction.uns['method_id']

        print("> Checking .uns['metric_ids']", flush=True)
        assert 'metric_ids' in output.uns
        assert set(output.uns['metric_ids']) == set(metric_meta.id)

        print("> Checking .uns['metric_values']", flush=True)
        assert 'metric_values' in output.uns
        assert output.uns['metric_ids'].size == output.uns['metric_values'].size

        # merge with metric_meta to see if metric_value lies within the expected range
        output_uns = pd.DataFrame({
          'id': output.uns['metric_ids'], 
          'value': output.uns['metric_values']
        })

        scores = metric_meta.merge(output_uns, on="id")

        assert all(scores.value >= scores['min'])
        assert all(scores.value <= scores['max'])

        print("> Test succeeded!", flush=True)
