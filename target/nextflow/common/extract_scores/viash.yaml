functionality:
  name: "extract_scores"
  namespace: "common"
  version: "main_build"
  authors: []
  inputs: []
  outputs: []
  arguments:
  - type: "file"
    name: "--input"
    alternatives:
    - "-i"
    description: "Input h5ad files containing metadata and metrics in adata.uns"
    example: []
    default:
    - "input.h5ad"
    must_exist: false
    required: false
    direction: "input"
    multiple: true
    multiple_sep: ":"
    dest: "par"
  - type: "string"
    name: "--column_names"
    alternatives: []
    description: "Which fields from adata.uns to extract and store as a data frame."
    example: []
    default:
    - "dataset_id"
    - "method_id"
    - "metric_id"
    - "metric_value"
    required: false
    choices: []
    direction: "input"
    multiple: true
    multiple_sep: ":"
    dest: "par"
  - type: "file"
    name: "--output"
    alternatives:
    - "-o"
    description: "Output tsv"
    example: []
    default:
    - "output.tsv"
    must_exist: false
    required: false
    direction: "output"
    multiple: false
    multiple_sep: ":"
    dest: "par"
  argument_groups: []
  resources:
  - type: "r_script"
    path: "script.R"
    is_executable: true
  description: "Extract evaluation data frame on output"
  test_resources: []
  status: "enabled"
  requirements:
    commands: []
  set_wd_to_resources_dir: false
platforms: []
info:
  config: "src/common/extract_scores/config.vsh.yaml"
  platform: "nextflow"
  output: "target/nextflow/common/extract_scores"
  viash_version: "0.6.3"
  git_commit: "87a6d699e013b9f4370c1bafdb9481709df87f2f"
  git_remote: "https://github.com/openproblems-bio/openproblems-v2"
